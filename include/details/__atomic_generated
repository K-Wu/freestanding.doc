/*

Copyright (c) 2018, NVIDIA Corporation

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.

*/

namespace simt { namespace details { inline namespace v1 {
static inline __device__ void __simt_fence_sc_block() { asm volatile("fence.sc.cta;":::"memory"); }
static inline __device__ void __simt_fence_acq_rel_block() { asm volatile("fence.acq_rel.cta;":::"memory"); }
static inline __device__ void __atomic_thread_fence_simt(int memorder, __thread_scope_block_tag) {
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block(); break;
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE:
    case __ATOMIC_ACQ_REL:
    case __ATOMIC_RELEASE: __simt_fence_acq_rel_block(); break;
    case __ATOMIC_RELAXED: break;
    default: assert(0);
    }
}
template<class _A, class _B> static inline __device__ void __simt_load_relaxed_8_block(_A _ptr, _B& _dst) {asm volatile("ld.relaxed.cta.b8 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_load_acquire_8_block(_A _ptr, _B& _dst) {asm volatile("ld.acquire.cta.b8 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename std::enable_if<sizeof(type)==1, int>::type = 0>
__device__ void __atomic_load_simt(const volatile type *ptr, type *ret, int memorder, __thread_scope_block_tag) {
    uint32_t tmp = 0;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_load_acquire_8_block(ptr, tmp); break;
    case __ATOMIC_RELAXED: __simt_load_relaxed_8_block(ptr, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 1);
}
template<class _A, class _B> static inline __device__ void __simt_load_relaxed_16_block(_A _ptr, _B& _dst) {asm volatile("ld.relaxed.cta.b16 %0,[%1];" : "=h"(_dst) : "l"(_ptr) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_load_acquire_16_block(_A _ptr, _B& _dst) {asm volatile("ld.acquire.cta.b16 %0,[%1];" : "=h"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename std::enable_if<sizeof(type)==2, int>::type = 0>
__device__ void __atomic_load_simt(const volatile type *ptr, type *ret, int memorder, __thread_scope_block_tag) {
    uint16_t tmp = 0;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_load_acquire_16_block(ptr, tmp); break;
    case __ATOMIC_RELAXED: __simt_load_relaxed_16_block(ptr, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 2);
}
template<class _A, class _B> static inline __device__ void __simt_load_relaxed_32_block(_A _ptr, _B& _dst) {asm volatile("ld.relaxed.cta.b32 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_load_acquire_32_block(_A _ptr, _B& _dst) {asm volatile("ld.acquire.cta.b32 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_load_simt(const volatile type *ptr, type *ret, int memorder, __thread_scope_block_tag) {
    uint32_t tmp = 0;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_load_acquire_32_block(ptr, tmp); break;
    case __ATOMIC_RELAXED: __simt_load_relaxed_32_block(ptr, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 4);
}
template<class _A, class _B> static inline __device__ void __simt_load_relaxed_64_block(_A _ptr, _B& _dst) {asm volatile("ld.relaxed.cta.b64 %0,[%1];" : "=l"(_dst) : "l"(_ptr) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_load_acquire_64_block(_A _ptr, _B& _dst) {asm volatile("ld.acquire.cta.b64 %0,[%1];" : "=l"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_load_simt(const volatile type *ptr, type *ret, int memorder, __thread_scope_block_tag) {
    uint64_t tmp = 0;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_load_acquire_64_block(ptr, tmp); break;
    case __ATOMIC_RELAXED: __simt_load_relaxed_64_block(ptr, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 8);
}
template<class _A, class _B> static inline __device__ void __simt_store_relaxed_8_block(_A _ptr, _B _src) { asm volatile("st.relaxed.cta.b8 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_store_release_8_block(_A _ptr, _B _src) { asm volatile("st.release.cta.b8 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==1, int>::type = 0>
__device__ void __atomic_store_simt(volatile type *ptr, type *val, int memorder, __thread_scope_block_tag) {
    uint32_t tmp = 0;
    memcpy(&tmp, val, 1);
    switch (memorder) {
    case __ATOMIC_RELEASE: __simt_store_release_8_block(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_RELAXED: __simt_store_relaxed_8_block(ptr, tmp); break;
    default: assert(0);
    }
}
template<class _A, class _B> static inline __device__ void __simt_store_relaxed_16_block(_A _ptr, _B _src) { asm volatile("st.relaxed.cta.b16 [%0], %1;" :: "l"(_ptr),"h"(_src) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_store_release_16_block(_A _ptr, _B _src) { asm volatile("st.release.cta.b16 [%0], %1;" :: "l"(_ptr),"h"(_src) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==2, int>::type = 0>
__device__ void __atomic_store_simt(volatile type *ptr, type *val, int memorder, __thread_scope_block_tag) {
    uint16_t tmp = 0;
    memcpy(&tmp, val, 2);
    switch (memorder) {
    case __ATOMIC_RELEASE: __simt_store_release_16_block(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_RELAXED: __simt_store_relaxed_16_block(ptr, tmp); break;
    default: assert(0);
    }
}
template<class _A, class _B> static inline __device__ void __simt_store_relaxed_32_block(_A _ptr, _B _src) { asm volatile("st.relaxed.cta.b32 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_store_release_32_block(_A _ptr, _B _src) { asm volatile("st.release.cta.b32 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_store_simt(volatile type *ptr, type *val, int memorder, __thread_scope_block_tag) {
    uint32_t tmp = 0;
    memcpy(&tmp, val, 4);
    switch (memorder) {
    case __ATOMIC_RELEASE: __simt_store_release_32_block(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_RELAXED: __simt_store_relaxed_32_block(ptr, tmp); break;
    default: assert(0);
    }
}
template<class _A, class _B> static inline __device__ void __simt_store_relaxed_64_block(_A _ptr, _B _src) { asm volatile("st.relaxed.cta.b64 [%0], %1;" :: "l"(_ptr),"l"(_src) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_store_release_64_block(_A _ptr, _B _src) { asm volatile("st.release.cta.b64 [%0], %1;" :: "l"(_ptr),"l"(_src) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_store_simt(volatile type *ptr, type *val, int memorder, __thread_scope_block_tag) {
    uint64_t tmp = 0;
    memcpy(&tmp, val, 8);
    switch (memorder) {
    case __ATOMIC_RELEASE: __simt_store_release_64_block(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_RELAXED: __simt_store_relaxed_64_block(ptr, tmp); break;
    default: assert(0);
    }
}
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_cas_relaxed_32_block(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.relaxed.cta.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_cas_acquire_32_block(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.acquire.cta.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_cas_release_32_block(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.release.cta.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_cas_acq_rel_32_block(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.acq_rel.cta.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ bool __atomic_compare_exchange_simt(volatile type *ptr, type *expected, const type *desired, bool, int success_memorder, int failure_memorder, __thread_scope_block_tag) {
    uint32_t tmp = 0, old = 0, old_tmp;
    memcpy(&tmp, desired, 4);
    memcpy(&old, expected, 4);
    old_tmp = old;
    switch (__stronger_order_simt(success_memorder, failure_memorder)) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_cas_acquire_32_block(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_cas_acq_rel_32_block(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_cas_release_32_block(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_cas_relaxed_32_block(ptr, old, old_tmp, tmp); break;
    default: assert(0);
    }
    bool const ret = old == old_tmp;
    memcpy(expected, &old, 4);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_exch_relaxed_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.relaxed.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exch_acquire_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.acquire.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exch_release_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.release.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exch_acq_rel_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.acq_rel.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_exchange_simt(volatile type *ptr, type *val, type *ret, int memorder, __thread_scope_block_tag) {
    uint32_t tmp = 0;
    memcpy(&tmp, val, 4);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_exch_acquire_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_exch_acq_rel_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_exch_release_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_exch_relaxed_32_block(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 4);
}
template<class _A, class _B, class _C> static inline __device__ void __simt_add_relaxed_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.relaxed.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_add_acquire_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.acquire.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_add_release_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.release.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_add_acq_rel_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.acq_rel.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_add_simt(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_add_acquire_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_add_acq_rel_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_add_release_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_add_relaxed_32_block(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_and_relaxed_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.relaxed.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_and_acquire_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.acquire.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_and_release_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.release.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_and_acq_rel_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.acq_rel.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_and_simt(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_and_acquire_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_and_acq_rel_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_and_release_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_and_relaxed_32_block(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_or_relaxed_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.relaxed.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_or_acquire_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.acquire.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_or_release_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.release.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_or_acq_rel_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.acq_rel.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_or_simt(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_or_acquire_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_or_acq_rel_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_or_release_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_or_relaxed_32_block(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_sub_simt(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    tmp = -tmp;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_add_acquire_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_add_acq_rel_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_add_release_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_add_relaxed_32_block(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_xor_relaxed_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.relaxed.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_xor_acquire_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.acquire.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_xor_release_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.release.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_xor_acq_rel_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.acq_rel.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_xor_simt(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_xor_acquire_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_xor_acq_rel_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_xor_release_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_xor_relaxed_32_block(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_cas_relaxed_64_block(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.relaxed.cta.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_cas_acquire_64_block(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.acquire.cta.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_cas_release_64_block(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.release.cta.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_cas_acq_rel_64_block(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.acq_rel.cta.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ bool __atomic_compare_exchange_simt(volatile type *ptr, type *expected, const type *desired, bool, int success_memorder, int failure_memorder, __thread_scope_block_tag) {
    uint64_t tmp = 0, old = 0, old_tmp;
    memcpy(&tmp, desired, 8);
    memcpy(&old, expected, 8);
    old_tmp = old;
    switch (__stronger_order_simt(success_memorder, failure_memorder)) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_cas_acquire_64_block(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_cas_acq_rel_64_block(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_cas_release_64_block(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_cas_relaxed_64_block(ptr, old, old_tmp, tmp); break;
    default: assert(0);
    }
    bool const ret = old == old_tmp;
    memcpy(expected, &old, 8);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_exch_relaxed_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.relaxed.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exch_acquire_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.acquire.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exch_release_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.release.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exch_acq_rel_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.acq_rel.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_exchange_simt(volatile type *ptr, type *val, type *ret, int memorder, __thread_scope_block_tag) {
    uint64_t tmp = 0;
    memcpy(&tmp, val, 8);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_exch_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_exch_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_exch_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_exch_relaxed_64_block(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 8);
}
template<class _A, class _B, class _C> static inline __device__ void __simt_add_relaxed_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.relaxed.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_add_acquire_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.acquire.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_add_release_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.release.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_add_acq_rel_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.acq_rel.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_add_simt(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_add_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_add_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_add_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_add_relaxed_64_block(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_and_relaxed_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.relaxed.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_and_acquire_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.acquire.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_and_release_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.release.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_and_acq_rel_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.acq_rel.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_and_simt(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_and_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_and_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_and_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_and_relaxed_64_block(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_or_relaxed_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.relaxed.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_or_acquire_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.acquire.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_or_release_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.release.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_or_acq_rel_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.acq_rel.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_or_simt(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_or_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_or_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_or_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_or_relaxed_64_block(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_sub_simt(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    tmp = -tmp;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_add_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_add_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_add_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_add_relaxed_64_block(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_xor_relaxed_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.relaxed.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_xor_acquire_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.acquire.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_xor_release_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.release.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_xor_acq_rel_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.acq_rel.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_xor_simt(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_xor_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_xor_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_xor_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_xor_relaxed_64_block(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class type>
__device__ type* __atomic_fetch_add_simt(type *volatile *ptr, ptrdiff_t val, int memorder, __thread_scope_block_tag) {
    type* ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    tmp *= sizeof(type);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_add_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_add_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_add_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_add_relaxed_64_block(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class type>
__device__ type* __atomic_fetch_sub_simt(type *volatile *ptr, ptrdiff_t val, int memorder, __thread_scope_block_tag) {
    type* ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    tmp = -tmp;
    tmp *= sizeof(type);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_add_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_add_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_add_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_add_relaxed_64_block(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
static inline __device__ void __simt_fence_sc_device() { asm volatile("fence.sc.gpu;":::"memory"); }
static inline __device__ void __simt_fence_acq_rel_device() { asm volatile("fence.acq_rel.gpu;":::"memory"); }
static inline __device__ void __atomic_thread_fence_simt(int memorder, __thread_scope_device_tag) {
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device(); break;
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE:
    case __ATOMIC_ACQ_REL:
    case __ATOMIC_RELEASE: __simt_fence_acq_rel_device(); break;
    case __ATOMIC_RELAXED: break;
    default: assert(0);
    }
}
template<class _A, class _B> static inline __device__ void __simt_load_relaxed_8_device(_A _ptr, _B& _dst) {asm volatile("ld.relaxed.gpu.b8 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_load_acquire_8_device(_A _ptr, _B& _dst) {asm volatile("ld.acquire.gpu.b8 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename std::enable_if<sizeof(type)==1, int>::type = 0>
__device__ void __atomic_load_simt(const volatile type *ptr, type *ret, int memorder, __thread_scope_device_tag) {
    uint32_t tmp = 0;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_load_acquire_8_device(ptr, tmp); break;
    case __ATOMIC_RELAXED: __simt_load_relaxed_8_device(ptr, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 1);
}
template<class _A, class _B> static inline __device__ void __simt_load_relaxed_16_device(_A _ptr, _B& _dst) {asm volatile("ld.relaxed.gpu.b16 %0,[%1];" : "=h"(_dst) : "l"(_ptr) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_load_acquire_16_device(_A _ptr, _B& _dst) {asm volatile("ld.acquire.gpu.b16 %0,[%1];" : "=h"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename std::enable_if<sizeof(type)==2, int>::type = 0>
__device__ void __atomic_load_simt(const volatile type *ptr, type *ret, int memorder, __thread_scope_device_tag) {
    uint16_t tmp = 0;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_load_acquire_16_device(ptr, tmp); break;
    case __ATOMIC_RELAXED: __simt_load_relaxed_16_device(ptr, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 2);
}
template<class _A, class _B> static inline __device__ void __simt_load_relaxed_32_device(_A _ptr, _B& _dst) {asm volatile("ld.relaxed.gpu.b32 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_load_acquire_32_device(_A _ptr, _B& _dst) {asm volatile("ld.acquire.gpu.b32 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_load_simt(const volatile type *ptr, type *ret, int memorder, __thread_scope_device_tag) {
    uint32_t tmp = 0;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_load_acquire_32_device(ptr, tmp); break;
    case __ATOMIC_RELAXED: __simt_load_relaxed_32_device(ptr, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 4);
}
template<class _A, class _B> static inline __device__ void __simt_load_relaxed_64_device(_A _ptr, _B& _dst) {asm volatile("ld.relaxed.gpu.b64 %0,[%1];" : "=l"(_dst) : "l"(_ptr) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_load_acquire_64_device(_A _ptr, _B& _dst) {asm volatile("ld.acquire.gpu.b64 %0,[%1];" : "=l"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_load_simt(const volatile type *ptr, type *ret, int memorder, __thread_scope_device_tag) {
    uint64_t tmp = 0;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_load_acquire_64_device(ptr, tmp); break;
    case __ATOMIC_RELAXED: __simt_load_relaxed_64_device(ptr, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 8);
}
template<class _A, class _B> static inline __device__ void __simt_store_relaxed_8_device(_A _ptr, _B _src) { asm volatile("st.relaxed.gpu.b8 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_store_release_8_device(_A _ptr, _B _src) { asm volatile("st.release.gpu.b8 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==1, int>::type = 0>
__device__ void __atomic_store_simt(volatile type *ptr, type *val, int memorder, __thread_scope_device_tag) {
    uint32_t tmp = 0;
    memcpy(&tmp, val, 1);
    switch (memorder) {
    case __ATOMIC_RELEASE: __simt_store_release_8_device(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_RELAXED: __simt_store_relaxed_8_device(ptr, tmp); break;
    default: assert(0);
    }
}
template<class _A, class _B> static inline __device__ void __simt_store_relaxed_16_device(_A _ptr, _B _src) { asm volatile("st.relaxed.gpu.b16 [%0], %1;" :: "l"(_ptr),"h"(_src) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_store_release_16_device(_A _ptr, _B _src) { asm volatile("st.release.gpu.b16 [%0], %1;" :: "l"(_ptr),"h"(_src) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==2, int>::type = 0>
__device__ void __atomic_store_simt(volatile type *ptr, type *val, int memorder, __thread_scope_device_tag) {
    uint16_t tmp = 0;
    memcpy(&tmp, val, 2);
    switch (memorder) {
    case __ATOMIC_RELEASE: __simt_store_release_16_device(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_RELAXED: __simt_store_relaxed_16_device(ptr, tmp); break;
    default: assert(0);
    }
}
template<class _A, class _B> static inline __device__ void __simt_store_relaxed_32_device(_A _ptr, _B _src) { asm volatile("st.relaxed.gpu.b32 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_store_release_32_device(_A _ptr, _B _src) { asm volatile("st.release.gpu.b32 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_store_simt(volatile type *ptr, type *val, int memorder, __thread_scope_device_tag) {
    uint32_t tmp = 0;
    memcpy(&tmp, val, 4);
    switch (memorder) {
    case __ATOMIC_RELEASE: __simt_store_release_32_device(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_RELAXED: __simt_store_relaxed_32_device(ptr, tmp); break;
    default: assert(0);
    }
}
template<class _A, class _B> static inline __device__ void __simt_store_relaxed_64_device(_A _ptr, _B _src) { asm volatile("st.relaxed.gpu.b64 [%0], %1;" :: "l"(_ptr),"l"(_src) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_store_release_64_device(_A _ptr, _B _src) { asm volatile("st.release.gpu.b64 [%0], %1;" :: "l"(_ptr),"l"(_src) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_store_simt(volatile type *ptr, type *val, int memorder, __thread_scope_device_tag) {
    uint64_t tmp = 0;
    memcpy(&tmp, val, 8);
    switch (memorder) {
    case __ATOMIC_RELEASE: __simt_store_release_64_device(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_RELAXED: __simt_store_relaxed_64_device(ptr, tmp); break;
    default: assert(0);
    }
}
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_cas_relaxed_32_device(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.relaxed.gpu.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_cas_acquire_32_device(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.acquire.gpu.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_cas_release_32_device(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.release.gpu.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_cas_acq_rel_32_device(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.acq_rel.gpu.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ bool __atomic_compare_exchange_simt(volatile type *ptr, type *expected, const type *desired, bool, int success_memorder, int failure_memorder, __thread_scope_device_tag) {
    uint32_t tmp = 0, old = 0, old_tmp;
    memcpy(&tmp, desired, 4);
    memcpy(&old, expected, 4);
    old_tmp = old;
    switch (__stronger_order_simt(success_memorder, failure_memorder)) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_cas_acquire_32_device(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_cas_acq_rel_32_device(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_cas_release_32_device(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_cas_relaxed_32_device(ptr, old, old_tmp, tmp); break;
    default: assert(0);
    }
    bool const ret = old == old_tmp;
    memcpy(expected, &old, 4);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_exch_relaxed_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.relaxed.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exch_acquire_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.acquire.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exch_release_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.release.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exch_acq_rel_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.acq_rel.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_exchange_simt(volatile type *ptr, type *val, type *ret, int memorder, __thread_scope_device_tag) {
    uint32_t tmp = 0;
    memcpy(&tmp, val, 4);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_exch_acquire_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_exch_acq_rel_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_exch_release_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_exch_relaxed_32_device(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 4);
}
template<class _A, class _B, class _C> static inline __device__ void __simt_add_relaxed_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.relaxed.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_add_acquire_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.acquire.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_add_release_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.release.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_add_acq_rel_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.acq_rel.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_add_simt(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_add_acquire_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_add_acq_rel_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_add_release_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_add_relaxed_32_device(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_and_relaxed_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.relaxed.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_and_acquire_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.acquire.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_and_release_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.release.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_and_acq_rel_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.acq_rel.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_and_simt(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_and_acquire_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_and_acq_rel_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_and_release_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_and_relaxed_32_device(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_or_relaxed_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.relaxed.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_or_acquire_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.acquire.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_or_release_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.release.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_or_acq_rel_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.acq_rel.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_or_simt(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_or_acquire_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_or_acq_rel_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_or_release_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_or_relaxed_32_device(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_sub_simt(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    tmp = -tmp;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_add_acquire_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_add_acq_rel_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_add_release_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_add_relaxed_32_device(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_xor_relaxed_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.relaxed.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_xor_acquire_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.acquire.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_xor_release_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.release.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_xor_acq_rel_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.acq_rel.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_xor_simt(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_xor_acquire_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_xor_acq_rel_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_xor_release_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_xor_relaxed_32_device(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_cas_relaxed_64_device(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.relaxed.gpu.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_cas_acquire_64_device(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.acquire.gpu.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_cas_release_64_device(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.release.gpu.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_cas_acq_rel_64_device(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.acq_rel.gpu.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ bool __atomic_compare_exchange_simt(volatile type *ptr, type *expected, const type *desired, bool, int success_memorder, int failure_memorder, __thread_scope_device_tag) {
    uint64_t tmp = 0, old = 0, old_tmp;
    memcpy(&tmp, desired, 8);
    memcpy(&old, expected, 8);
    old_tmp = old;
    switch (__stronger_order_simt(success_memorder, failure_memorder)) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_cas_acquire_64_device(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_cas_acq_rel_64_device(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_cas_release_64_device(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_cas_relaxed_64_device(ptr, old, old_tmp, tmp); break;
    default: assert(0);
    }
    bool const ret = old == old_tmp;
    memcpy(expected, &old, 8);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_exch_relaxed_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.relaxed.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exch_acquire_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.acquire.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exch_release_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.release.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exch_acq_rel_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.acq_rel.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_exchange_simt(volatile type *ptr, type *val, type *ret, int memorder, __thread_scope_device_tag) {
    uint64_t tmp = 0;
    memcpy(&tmp, val, 8);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_exch_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_exch_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_exch_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_exch_relaxed_64_device(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 8);
}
template<class _A, class _B, class _C> static inline __device__ void __simt_add_relaxed_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.relaxed.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_add_acquire_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.acquire.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_add_release_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.release.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_add_acq_rel_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.acq_rel.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_add_simt(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_add_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_add_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_add_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_add_relaxed_64_device(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_and_relaxed_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.relaxed.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_and_acquire_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.acquire.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_and_release_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.release.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_and_acq_rel_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.acq_rel.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_and_simt(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_and_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_and_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_and_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_and_relaxed_64_device(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_or_relaxed_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.relaxed.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_or_acquire_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.acquire.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_or_release_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.release.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_or_acq_rel_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.acq_rel.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_or_simt(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_or_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_or_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_or_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_or_relaxed_64_device(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_sub_simt(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    tmp = -tmp;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_add_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_add_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_add_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_add_relaxed_64_device(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_xor_relaxed_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.relaxed.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_xor_acquire_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.acquire.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_xor_release_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.release.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_xor_acq_rel_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.acq_rel.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_xor_simt(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_xor_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_xor_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_xor_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_xor_relaxed_64_device(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class type>
__device__ type* __atomic_fetch_add_simt(type *volatile *ptr, ptrdiff_t val, int memorder, __thread_scope_device_tag) {
    type* ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    tmp *= sizeof(type);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_add_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_add_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_add_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_add_relaxed_64_device(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class type>
__device__ type* __atomic_fetch_sub_simt(type *volatile *ptr, ptrdiff_t val, int memorder, __thread_scope_device_tag) {
    type* ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    tmp = -tmp;
    tmp *= sizeof(type);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_add_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_add_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_add_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_add_relaxed_64_device(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
static inline __device__ void __simt_fence_sc_system() { asm volatile("fence.sc.sys;":::"memory"); }
static inline __device__ void __simt_fence_acq_rel_system() { asm volatile("fence.acq_rel.sys;":::"memory"); }
static inline __device__ void __atomic_thread_fence_simt(int memorder, __thread_scope_system_tag) {
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system(); break;
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE:
    case __ATOMIC_ACQ_REL:
    case __ATOMIC_RELEASE: __simt_fence_acq_rel_system(); break;
    case __ATOMIC_RELAXED: break;
    default: assert(0);
    }
}
template<class _A, class _B> static inline __device__ void __simt_load_relaxed_8_system(_A _ptr, _B& _dst) {asm volatile("ld.relaxed.sys.b8 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_load_acquire_8_system(_A _ptr, _B& _dst) {asm volatile("ld.acquire.sys.b8 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename std::enable_if<sizeof(type)==1, int>::type = 0>
__device__ void __atomic_load_simt(const volatile type *ptr, type *ret, int memorder, __thread_scope_system_tag) {
    uint32_t tmp = 0;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_load_acquire_8_system(ptr, tmp); break;
    case __ATOMIC_RELAXED: __simt_load_relaxed_8_system(ptr, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 1);
}
template<class _A, class _B> static inline __device__ void __simt_load_relaxed_16_system(_A _ptr, _B& _dst) {asm volatile("ld.relaxed.sys.b16 %0,[%1];" : "=h"(_dst) : "l"(_ptr) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_load_acquire_16_system(_A _ptr, _B& _dst) {asm volatile("ld.acquire.sys.b16 %0,[%1];" : "=h"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename std::enable_if<sizeof(type)==2, int>::type = 0>
__device__ void __atomic_load_simt(const volatile type *ptr, type *ret, int memorder, __thread_scope_system_tag) {
    uint16_t tmp = 0;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_load_acquire_16_system(ptr, tmp); break;
    case __ATOMIC_RELAXED: __simt_load_relaxed_16_system(ptr, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 2);
}
template<class _A, class _B> static inline __device__ void __simt_load_relaxed_32_system(_A _ptr, _B& _dst) {asm volatile("ld.relaxed.sys.b32 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_load_acquire_32_system(_A _ptr, _B& _dst) {asm volatile("ld.acquire.sys.b32 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_load_simt(const volatile type *ptr, type *ret, int memorder, __thread_scope_system_tag) {
    uint32_t tmp = 0;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_load_acquire_32_system(ptr, tmp); break;
    case __ATOMIC_RELAXED: __simt_load_relaxed_32_system(ptr, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 4);
}
template<class _A, class _B> static inline __device__ void __simt_load_relaxed_64_system(_A _ptr, _B& _dst) {asm volatile("ld.relaxed.sys.b64 %0,[%1];" : "=l"(_dst) : "l"(_ptr) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_load_acquire_64_system(_A _ptr, _B& _dst) {asm volatile("ld.acquire.sys.b64 %0,[%1];" : "=l"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_load_simt(const volatile type *ptr, type *ret, int memorder, __thread_scope_system_tag) {
    uint64_t tmp = 0;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_load_acquire_64_system(ptr, tmp); break;
    case __ATOMIC_RELAXED: __simt_load_relaxed_64_system(ptr, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 8);
}
template<class _A, class _B> static inline __device__ void __simt_store_relaxed_8_system(_A _ptr, _B _src) { asm volatile("st.relaxed.sys.b8 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_store_release_8_system(_A _ptr, _B _src) { asm volatile("st.release.sys.b8 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==1, int>::type = 0>
__device__ void __atomic_store_simt(volatile type *ptr, type *val, int memorder, __thread_scope_system_tag) {
    uint32_t tmp = 0;
    memcpy(&tmp, val, 1);
    switch (memorder) {
    case __ATOMIC_RELEASE: __simt_store_release_8_system(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_RELAXED: __simt_store_relaxed_8_system(ptr, tmp); break;
    default: assert(0);
    }
}
template<class _A, class _B> static inline __device__ void __simt_store_relaxed_16_system(_A _ptr, _B _src) { asm volatile("st.relaxed.sys.b16 [%0], %1;" :: "l"(_ptr),"h"(_src) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_store_release_16_system(_A _ptr, _B _src) { asm volatile("st.release.sys.b16 [%0], %1;" :: "l"(_ptr),"h"(_src) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==2, int>::type = 0>
__device__ void __atomic_store_simt(volatile type *ptr, type *val, int memorder, __thread_scope_system_tag) {
    uint16_t tmp = 0;
    memcpy(&tmp, val, 2);
    switch (memorder) {
    case __ATOMIC_RELEASE: __simt_store_release_16_system(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_RELAXED: __simt_store_relaxed_16_system(ptr, tmp); break;
    default: assert(0);
    }
}
template<class _A, class _B> static inline __device__ void __simt_store_relaxed_32_system(_A _ptr, _B _src) { asm volatile("st.relaxed.sys.b32 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_store_release_32_system(_A _ptr, _B _src) { asm volatile("st.release.sys.b32 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_store_simt(volatile type *ptr, type *val, int memorder, __thread_scope_system_tag) {
    uint32_t tmp = 0;
    memcpy(&tmp, val, 4);
    switch (memorder) {
    case __ATOMIC_RELEASE: __simt_store_release_32_system(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_RELAXED: __simt_store_relaxed_32_system(ptr, tmp); break;
    default: assert(0);
    }
}
template<class _A, class _B> static inline __device__ void __simt_store_relaxed_64_system(_A _ptr, _B _src) { asm volatile("st.relaxed.sys.b64 [%0], %1;" :: "l"(_ptr),"l"(_src) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_store_release_64_system(_A _ptr, _B _src) { asm volatile("st.release.sys.b64 [%0], %1;" :: "l"(_ptr),"l"(_src) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_store_simt(volatile type *ptr, type *val, int memorder, __thread_scope_system_tag) {
    uint64_t tmp = 0;
    memcpy(&tmp, val, 8);
    switch (memorder) {
    case __ATOMIC_RELEASE: __simt_store_release_64_system(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_RELAXED: __simt_store_relaxed_64_system(ptr, tmp); break;
    default: assert(0);
    }
}
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_cas_relaxed_32_system(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.relaxed.sys.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_cas_acquire_32_system(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.acquire.sys.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_cas_release_32_system(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.release.sys.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_cas_acq_rel_32_system(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.acq_rel.sys.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ bool __atomic_compare_exchange_simt(volatile type *ptr, type *expected, const type *desired, bool, int success_memorder, int failure_memorder, __thread_scope_system_tag) {
    uint32_t tmp = 0, old = 0, old_tmp;
    memcpy(&tmp, desired, 4);
    memcpy(&old, expected, 4);
    old_tmp = old;
    switch (__stronger_order_simt(success_memorder, failure_memorder)) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_cas_acquire_32_system(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_cas_acq_rel_32_system(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_cas_release_32_system(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_cas_relaxed_32_system(ptr, old, old_tmp, tmp); break;
    default: assert(0);
    }
    bool const ret = old == old_tmp;
    memcpy(expected, &old, 4);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_exch_relaxed_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.relaxed.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exch_acquire_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.acquire.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exch_release_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.release.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exch_acq_rel_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.acq_rel.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_exchange_simt(volatile type *ptr, type *val, type *ret, int memorder, __thread_scope_system_tag) {
    uint32_t tmp = 0;
    memcpy(&tmp, val, 4);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_exch_acquire_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_exch_acq_rel_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_exch_release_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_exch_relaxed_32_system(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 4);
}
template<class _A, class _B, class _C> static inline __device__ void __simt_add_relaxed_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.relaxed.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_add_acquire_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.acquire.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_add_release_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.release.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_add_acq_rel_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.acq_rel.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_add_simt(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_add_acquire_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_add_acq_rel_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_add_release_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_add_relaxed_32_system(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_and_relaxed_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.relaxed.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_and_acquire_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.acquire.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_and_release_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.release.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_and_acq_rel_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.acq_rel.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_and_simt(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_and_acquire_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_and_acq_rel_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_and_release_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_and_relaxed_32_system(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_or_relaxed_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.relaxed.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_or_acquire_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.acquire.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_or_release_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.release.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_or_acq_rel_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.acq_rel.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_or_simt(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_or_acquire_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_or_acq_rel_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_or_release_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_or_relaxed_32_system(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_sub_simt(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    tmp = -tmp;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_add_acquire_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_add_acq_rel_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_add_release_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_add_relaxed_32_system(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_xor_relaxed_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.relaxed.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_xor_acquire_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.acquire.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_xor_release_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.release.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_xor_acq_rel_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.acq_rel.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_xor_simt(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_xor_acquire_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_xor_acq_rel_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_xor_release_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_xor_relaxed_32_system(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_cas_relaxed_64_system(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.relaxed.sys.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_cas_acquire_64_system(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.acquire.sys.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_cas_release_64_system(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.release.sys.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_cas_acq_rel_64_system(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.acq_rel.sys.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ bool __atomic_compare_exchange_simt(volatile type *ptr, type *expected, const type *desired, bool, int success_memorder, int failure_memorder, __thread_scope_system_tag) {
    uint64_t tmp = 0, old = 0, old_tmp;
    memcpy(&tmp, desired, 8);
    memcpy(&old, expected, 8);
    old_tmp = old;
    switch (__stronger_order_simt(success_memorder, failure_memorder)) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_cas_acquire_64_system(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_cas_acq_rel_64_system(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_cas_release_64_system(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_cas_relaxed_64_system(ptr, old, old_tmp, tmp); break;
    default: assert(0);
    }
    bool const ret = old == old_tmp;
    memcpy(expected, &old, 8);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_exch_relaxed_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.relaxed.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exch_acquire_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.acquire.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exch_release_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.release.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exch_acq_rel_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.acq_rel.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_exchange_simt(volatile type *ptr, type *val, type *ret, int memorder, __thread_scope_system_tag) {
    uint64_t tmp = 0;
    memcpy(&tmp, val, 8);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_exch_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_exch_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_exch_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_exch_relaxed_64_system(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(ret, &tmp, 8);
}
template<class _A, class _B, class _C> static inline __device__ void __simt_add_relaxed_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.relaxed.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_add_acquire_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.acquire.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_add_release_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.release.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_add_acq_rel_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.acq_rel.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_add_simt(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_add_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_add_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_add_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_add_relaxed_64_system(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_and_relaxed_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.relaxed.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_and_acquire_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.acquire.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_and_release_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.release.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_and_acq_rel_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.acq_rel.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_and_simt(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_and_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_and_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_and_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_and_relaxed_64_system(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_or_relaxed_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.relaxed.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_or_acquire_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.acquire.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_or_release_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.release.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_or_acq_rel_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.acq_rel.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_or_simt(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_or_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_or_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_or_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_or_relaxed_64_system(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_sub_simt(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    tmp = -tmp;
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_add_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_add_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_add_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_add_relaxed_64_system(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_xor_relaxed_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.relaxed.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_xor_acquire_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.acquire.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_xor_release_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.release.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_xor_acq_rel_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.acq_rel.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_xor_simt(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_xor_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_xor_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_xor_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_xor_relaxed_64_system(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class type>
__device__ type* __atomic_fetch_add_simt(type *volatile *ptr, ptrdiff_t val, int memorder, __thread_scope_system_tag) {
    type* ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    tmp *= sizeof(type);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_add_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_add_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_add_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_add_relaxed_64_system(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class type>
__device__ type* __atomic_fetch_sub_simt(type *volatile *ptr, ptrdiff_t val, int memorder, __thread_scope_system_tag) {
    type* ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    tmp = -tmp;
    tmp *= sizeof(type);
    switch (memorder) {
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_add_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_add_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_add_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_add_relaxed_64_system(ptr, tmp, tmp); break;
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
} } }
